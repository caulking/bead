"""HuggingFace model adapters for language models and NLI.

This module provides adapters for HuggingFace Transformers models:
- HuggingFaceLanguageModel: Causal LMs (GPT-2, GPT-Neo, Llama, Mistral)
- HuggingFaceMaskedLanguageModel: Masked LMs (BERT, RoBERTa, ALBERT)
- HuggingFaceNLI: NLI models (RoBERTa-MNLI, DeBERTa-MNLI, BART-MNLI)
"""

from __future__ import annotations

import logging
from typing import Any

import numpy as np
import torch
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoModelForMaskedLM,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    PreTrainedModel,
    PreTrainedTokenizerBase,
)

from sash.adapters.huggingface import DeviceType, HuggingFaceAdapterMixin
from sash.items.adapters.base import ModelAdapter
from sash.items.cache import ModelOutputCache

logger = logging.getLogger(__name__)


class HuggingFaceLanguageModel(HuggingFaceAdapterMixin, ModelAdapter):
    """Adapter for HuggingFace causal language models.

    Supports models like GPT-2, GPT-Neo, Llama, Mistral, and other
    autoregressive (left-to-right) language models.

    Parameters
    ----------
    model_name : str
        HuggingFace model identifier (e.g., "gpt2", "gpt2-medium").
    cache : ModelOutputCache
        Cache instance for storing model outputs.
    device : {"cpu", "cuda", "mps"}
        Device to run model on. Falls back to CPU if device unavailable.
    model_version : str
        Version string for cache tracking.

    Examples
    --------
    >>> from pathlib import Path
    >>> from sash.items.cache import ModelOutputCache
    >>> cache = ModelOutputCache(cache_dir=Path(".cache"))
    >>> model = HuggingFaceLanguageModel("gpt2", cache, device="cpu")
    >>> log_prob = model.compute_log_probability("The cat sat on the mat.")
    >>> perplexity = model.compute_perplexity("The cat sat on the mat.")
    >>> embedding = model.get_embedding("The cat sat on the mat.")
    """

    def __init__(
        self,
        model_name: str,
        cache: ModelOutputCache,
        device: DeviceType = "cpu",
        model_version: str = "unknown",
    ) -> None:
        """Initialize HuggingFace language model adapter.

        Parameters
        ----------
        model_name : str
            HuggingFace model identifier.
        cache : ModelOutputCache
            Cache instance.
        device : {"cpu", "cuda", "mps"}
            Device to run model on.
        model_version : str
            Version string for cache tracking.
        """
        super().__init__(model_name, cache, model_version)
        self.device = self._validate_device(device)
        self._model: PreTrainedModel | None = None
        self._tokenizer: PreTrainedTokenizerBase | None = None

    def _load_model(self) -> None:
        """Load model and tokenizer lazily on first use."""
        if self._model is None:
            logger.info(f"Loading causal LM: {self.model_name}")
            self._model = AutoModelForCausalLM.from_pretrained(self.model_name)
            self._model.to(self.device)
            self._model.eval()

        if self._tokenizer is None:
            self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            # Set padding token for models that don't have one
            if self._tokenizer.pad_token is None:
                self._tokenizer.pad_token = self._tokenizer.eos_token

    @property
    def model(self) -> PreTrainedModel:
        """Get the model, loading if necessary."""
        self._load_model()
        assert self._model is not None
        return self._model

    @property
    def tokenizer(self) -> PreTrainedTokenizerBase:
        """Get the tokenizer, loading if necessary."""
        self._load_model()
        assert self._tokenizer is not None
        return self._tokenizer

    def compute_log_probability(self, text: str) -> float:
        """Compute log probability of text under language model.

        Uses the model's loss with labels=input_ids to compute the negative
        log-likelihood of the text.

        Parameters
        ----------
        text : str
            Text to compute log probability for.

        Returns
        -------
        float
            Log probability of the text.
        """
        # Check cache
        cached = self.cache.get(self.model_name, "log_probability", text=text)
        if cached is not None:
            return cached

        # Tokenize
        inputs = self.tokenizer(
            text, return_tensors="pt", padding=True, truncation=True
        )
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        # Compute loss (negative log-likelihood)
        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids, attention_mask=attention_mask, labels=input_ids
            )
            loss = outputs.loss.item()

        # Loss is negative log-likelihood per token, convert to total log prob
        log_prob = -loss * input_ids.size(1)

        # Cache result
        self.cache.set(
            self.model_name,
            "log_probability",
            log_prob,
            model_version=self.model_version,
            text=text,
        )

        return log_prob

    def compute_perplexity(self, text: str) -> float:
        """Compute perplexity of text.

        Perplexity is exp(average negative log-likelihood per token).

        Parameters
        ----------
        text : str
            Text to compute perplexity for.

        Returns
        -------
        float
            Perplexity of the text (positive value).
        """
        # Check cache
        cached = self.cache.get(self.model_name, "perplexity", text=text)
        if cached is not None:
            return cached

        # Tokenize
        inputs = self.tokenizer(
            text, return_tensors="pt", padding=True, truncation=True
        )
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        # Compute loss
        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids, attention_mask=attention_mask, labels=input_ids
            )
            loss = outputs.loss.item()

        # Perplexity is exp(loss)
        perplexity = np.exp(loss)

        # Cache result
        self.cache.set(
            self.model_name,
            "perplexity",
            perplexity,
            model_version=self.model_version,
            text=text,
        )

        return float(perplexity)

    def get_embedding(self, text: str) -> np.ndarray:
        """Get embedding vector for text.

        Uses mean pooling of last hidden states as the text embedding.

        Parameters
        ----------
        text : str
            Text to embed.

        Returns
        -------
        np.ndarray
            Embedding vector for the text.
        """
        # Check cache
        cached = self.cache.get(self.model_name, "embedding", text=text)
        if cached is not None:
            return cached

        # Tokenize
        inputs = self.tokenizer(
            text, return_tensors="pt", padding=True, truncation=True
        )
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        # Get hidden states
        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True,
            )
            hidden_states = outputs.hidden_states[-1]  # Last layer

        # Mean pooling (weighted by attention mask)
        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size())
        sum_hidden = torch.sum(hidden_states * mask_expanded, dim=1)
        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)
        embedding = (sum_hidden / sum_mask).squeeze(0).cpu().numpy()

        # Cache result
        self.cache.set(
            self.model_name,
            "embedding",
            embedding,
            model_version=self.model_version,
            text=text,
        )

        return embedding

    def compute_nli(self, premise: str, hypothesis: str) -> dict[str, float]:
        """Compute natural language inference scores.

        Not supported for causal language models.

        Raises
        ------
        NotImplementedError
            Always raised, as causal LMs don't support NLI directly.
        """
        raise NotImplementedError(
            f"NLI is not supported for causal language model {self.model_name}. "
            "Use HuggingFaceNLI adapter with an NLI-trained model instead."
        )


class HuggingFaceMaskedLanguageModel(HuggingFaceAdapterMixin, ModelAdapter):
    """Adapter for HuggingFace masked language models.

    Supports models like BERT, RoBERTa, ALBERT, and other masked language
    models (MLMs).

    Parameters
    ----------
    model_name : str
        HuggingFace model identifier (e.g., "bert-base-uncased").
    cache : ModelOutputCache
        Cache instance for storing model outputs.
    device : {"cpu", "cuda", "mps"}
        Device to run model on. Falls back to CPU if device unavailable.
    model_version : str
        Version string for cache tracking.

    Examples
    --------
    >>> from pathlib import Path
    >>> from sash.items.cache import ModelOutputCache
    >>> cache = ModelOutputCache(cache_dir=Path(".cache"))
    >>> model = HuggingFaceMaskedLanguageModel("bert-base-uncased", cache)
    >>> log_prob = model.compute_log_probability("The cat sat on the mat.")
    >>> embedding = model.get_embedding("The cat sat on the mat.")
    """

    def __init__(
        self,
        model_name: str,
        cache: ModelOutputCache,
        device: DeviceType = "cpu",
        model_version: str = "unknown",
    ) -> None:
        """Initialize HuggingFace masked language model adapter.

        Parameters
        ----------
        model_name : str
            HuggingFace model identifier.
        cache : ModelOutputCache
            Cache instance.
        device : {"cpu", "cuda", "mps"}
            Device to run model on.
        model_version : str
            Version string for cache tracking.
        """
        super().__init__(model_name, cache, model_version)
        self.device = self._validate_device(device)
        self._model: PreTrainedModel | None = None
        self._tokenizer: PreTrainedTokenizerBase | None = None

    def _load_model(self) -> None:
        """Load model and tokenizer lazily on first use."""
        if self._model is None:
            logger.info(f"Loading masked LM: {self.model_name}")
            self._model = AutoModelForMaskedLM.from_pretrained(self.model_name)
            self._model.to(self.device)
            self._model.eval()

        if self._tokenizer is None:
            self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)

    @property
    def model(self) -> PreTrainedModel:
        """Get the model, loading if necessary."""
        self._load_model()
        assert self._model is not None
        return self._model

    @property
    def tokenizer(self) -> PreTrainedTokenizerBase:
        """Get the tokenizer, loading if necessary."""
        self._load_model()
        assert self._tokenizer is not None
        return self._tokenizer

    def compute_log_probability(self, text: str) -> float:
        """Compute log probability of text using pseudo-log-likelihood.

        For MLMs, we use pseudo-log-likelihood: mask each token one at a time
        and sum the log probabilities of predicting each token.

        This is computationally expensive - caching is critical.

        Parameters
        ----------
        text : str
            Text to compute log probability for.

        Returns
        -------
        float
            Pseudo-log-probability of the text.
        """
        # Check cache
        cached = self.cache.get(self.model_name, "log_probability", text=text)
        if cached is not None:
            return cached

        # Tokenize
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True)
        input_ids = inputs["input_ids"].to(self.device)

        # Compute pseudo-log-likelihood by masking each token
        total_log_prob = 0.0
        num_tokens = input_ids.size(1)

        with torch.no_grad():
            for i in range(num_tokens):
                # Skip special tokens
                if input_ids[0, i] in [
                    self.tokenizer.cls_token_id,
                    self.tokenizer.sep_token_id,
                    self.tokenizer.pad_token_id,
                ]:
                    continue

                # Create masked version
                masked_input = input_ids.clone()
                original_token = masked_input[0, i].item()
                masked_input[0, i] = self.tokenizer.mask_token_id

                # Get prediction
                outputs = self.model(masked_input)
                logits = outputs.logits[0, i]  # Logits for masked position

                # Compute log probability of original token
                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)
                total_log_prob += log_probs[original_token].item()

        # Cache result
        self.cache.set(
            self.model_name,
            "log_probability",
            total_log_prob,
            model_version=self.model_version,
            text=text,
        )

        return total_log_prob

    def compute_perplexity(self, text: str) -> float:
        """Compute perplexity based on pseudo-log-likelihood.

        Parameters
        ----------
        text : str
            Text to compute perplexity for.

        Returns
        -------
        float
            Perplexity of the text (positive value).
        """
        # Check cache
        cached = self.cache.get(self.model_name, "perplexity", text=text)
        if cached is not None:
            return cached

        # Get log probability
        log_prob = self.compute_log_probability(text)

        # Count non-special tokens
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True)
        input_ids = inputs["input_ids"]
        num_tokens = sum(
            1
            for token_id in input_ids[0].tolist()
            if token_id
            not in [
                self.tokenizer.cls_token_id,
                self.tokenizer.sep_token_id,
                self.tokenizer.pad_token_id,
            ]
        )

        # Perplexity is exp(-log_prob / num_tokens)
        perplexity = np.exp(-log_prob / max(num_tokens, 1))

        # Cache result
        self.cache.set(
            self.model_name,
            "perplexity",
            perplexity,
            model_version=self.model_version,
            text=text,
        )

        return float(perplexity)

    def get_embedding(self, text: str) -> np.ndarray:
        """Get embedding vector for text.

        Uses the [CLS] token embedding from the last layer.

        Parameters
        ----------
        text : str
            Text to embed.

        Returns
        -------
        np.ndarray
            Embedding vector for the text.
        """
        # Check cache
        cached = self.cache.get(self.model_name, "embedding", text=text)
        if cached is not None:
            return cached

        # Tokenize
        inputs = self.tokenizer(
            text, return_tensors="pt", padding=True, truncation=True
        )
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        # Get hidden states
        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True,
            )
            # Use [CLS] token from last layer
            hidden_states = outputs.hidden_states[-1]
            cls_embedding = hidden_states[0, 0].cpu().numpy()

        # Cache result
        self.cache.set(
            self.model_name,
            "embedding",
            cls_embedding,
            model_version=self.model_version,
            text=text,
        )

        return cls_embedding

    def compute_nli(self, premise: str, hypothesis: str) -> dict[str, float]:
        """Compute natural language inference scores.

        Not supported for masked language models.

        Raises
        ------
        NotImplementedError
            Always raised, as MLMs don't support NLI directly.
        """
        raise NotImplementedError(
            f"NLI is not supported for masked language model {self.model_name}. "
            "Use HuggingFaceNLI adapter with an NLI-trained model instead."
        )


class HuggingFaceNLI(HuggingFaceAdapterMixin, ModelAdapter):
    """Adapter for HuggingFace NLI models.

    Supports NLI models trained on MNLI and similar datasets
    (e.g., "roberta-large-mnli", "microsoft/deberta-base-mnli").

    Parameters
    ----------
    model_name : str
        HuggingFace model identifier for NLI model.
    cache : ModelOutputCache
        Cache instance for storing model outputs.
    device : {"cpu", "cuda", "mps"}
        Device to run model on. Falls back to CPU if device unavailable.
    model_version : str
        Version string for cache tracking.

    Examples
    --------
    >>> from pathlib import Path
    >>> from sash.items.cache import ModelOutputCache
    >>> cache = ModelOutputCache(cache_dir=Path(".cache"))
    >>> nli = HuggingFaceNLI("roberta-large-mnli", cache, device="cpu")
    >>> scores = nli.compute_nli(
    ...     premise="Mary loves reading books.",
    ...     hypothesis="Mary enjoys literature."
    ... )
    >>> label = nli.get_nli_label(
    ...     premise="Mary loves reading books.",
    ...     hypothesis="Mary enjoys literature."
    ... )
    """

    def __init__(
        self,
        model_name: str,
        cache: ModelOutputCache,
        device: DeviceType = "cpu",
        model_version: str = "unknown",
    ) -> None:
        """Initialize HuggingFace NLI adapter.

        Parameters
        ----------
        model_name : str
            HuggingFace model identifier.
        cache : ModelOutputCache
            Cache instance.
        device : {"cpu", "cuda", "mps"}
            Device to run model on.
        model_version : str
            Version string for cache tracking.
        """
        super().__init__(model_name, cache, model_version)
        self.device = self._validate_device(device)
        self._model: PreTrainedModel | None = None
        self._tokenizer: PreTrainedTokenizerBase | None = None
        self._label_mapping: dict[str, str] = {}

    def _load_model(self) -> None:
        """Load model and tokenizer lazily on first use."""
        if self._model is None:
            logger.info(f"Loading NLI model: {self.model_name}")
            self._model = AutoModelForSequenceClassification.from_pretrained(
                self.model_name
            )
            self._model.to(self.device)
            self._model.eval()

            # Get label mapping from config
            config = AutoConfig.from_pretrained(self.model_name)
            if hasattr(config, "id2label"):
                # Build mapping from model labels to standard labels
                self._label_mapping = self._build_label_mapping(config.id2label)
            else:
                # Default mapping (assume standard order)
                self._label_mapping = {
                    "0": "entailment",
                    "1": "neutral",
                    "2": "contradiction",
                }

        if self._tokenizer is None:
            self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)

    def _build_label_mapping(self, id2label: dict[Any, Any]) -> dict[str, str]:
        """Build mapping from model label IDs to standard NLI labels.

        Parameters
        ----------
        id2label : dict
            Mapping from label IDs to label strings from model config.

        Returns
        -------
        dict[str, str]
            Mapping from label IDs (as strings) to standard labels.
        """
        mapping: dict[str, str] = {}
        for idx, label in id2label.items():
            # Normalize label to lowercase
            normalized = label.lower()
            # Map to standard labels
            if "entail" in normalized:
                mapping[str(idx)] = "entailment"
            elif "neutral" in normalized:
                mapping[str(idx)] = "neutral"
            elif "contradict" in normalized:
                mapping[str(idx)] = "contradiction"
            else:
                # Keep original if we can't map it
                mapping[str(idx)] = normalized
        return mapping

    @property
    def model(self) -> PreTrainedModel:
        """Get the model, loading if necessary."""
        self._load_model()
        assert self._model is not None
        return self._model

    @property
    def tokenizer(self) -> PreTrainedTokenizerBase:
        """Get the tokenizer, loading if necessary."""
        self._load_model()
        assert self._tokenizer is not None
        return self._tokenizer

    def compute_log_probability(self, text: str) -> float:
        """Compute log probability of text.

        Not supported for NLI models.

        Raises
        ------
        NotImplementedError
            Always raised, as NLI models don't provide log probabilities.
        """
        raise NotImplementedError(
            f"Log probability is not supported for NLI model {self.model_name}. "
            "Use HuggingFaceLanguageModel or HuggingFaceMaskedLanguageModel instead."
        )

    def compute_perplexity(self, text: str) -> float:
        """Compute perplexity of text.

        Not supported for NLI models.

        Raises
        ------
        NotImplementedError
            Always raised, as NLI models don't provide perplexity.
        """
        raise NotImplementedError(
            f"Perplexity is not supported for NLI model {self.model_name}. "
            "Use HuggingFaceLanguageModel or HuggingFaceMaskedLanguageModel instead."
        )

    def get_embedding(self, text: str) -> np.ndarray:
        """Get embedding vector for text.

        Uses the model's encoder to get embeddings. Note that NLI models
        are typically fine-tuned for classification, so embeddings may not
        be optimal for general similarity tasks.

        Parameters
        ----------
        text : str
            Text to embed.

        Returns
        -------
        np.ndarray
            Embedding vector for the text.
        """
        # Check cache
        cached = self.cache.get(self.model_name, "embedding", text=text)
        if cached is not None:
            return cached

        # Tokenize
        inputs = self.tokenizer(
            text, return_tensors="pt", padding=True, truncation=True
        )
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        # Get hidden states (using base model if available)
        with torch.no_grad():
            # Try to access base model for embeddings
            if hasattr(self.model, "roberta"):
                base_model = self.model.roberta
            elif hasattr(self.model, "deberta"):
                base_model = self.model.deberta
            elif hasattr(self.model, "bert"):
                base_model = self.model.bert
            else:
                # Fallback: use full model with output_hidden_states
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    output_hidden_states=True,
                )
                hidden_states = outputs.hidden_states[-1]
                embedding = hidden_states[0, 0].cpu().numpy()
                self.cache.set(
                    self.model_name,
                    "embedding",
                    embedding,
                    model_version=self.model_version,
                    text=text,
                )
                return embedding

            # Use base model
            outputs = base_model(input_ids=input_ids, attention_mask=attention_mask)
            # Use [CLS] token
            embedding = outputs.last_hidden_state[0, 0].cpu().numpy()

        # Cache result
        self.cache.set(
            self.model_name,
            "embedding",
            embedding,
            model_version=self.model_version,
            text=text,
        )

        return embedding

    def compute_nli(self, premise: str, hypothesis: str) -> dict[str, float]:
        """Compute natural language inference scores.

        Parameters
        ----------
        premise : str
            Premise text.
        hypothesis : str
            Hypothesis text.

        Returns
        -------
        dict[str, float]
            Dictionary with keys "entailment", "neutral", "contradiction"
            mapping to probability scores that sum to ~1.0.
        """
        # Check cache
        cached = self.cache.get(
            self.model_name, "nli", premise=premise, hypothesis=hypothesis
        )
        if cached is not None:
            return cached

        # Tokenize premise-hypothesis pair
        inputs = self.tokenizer(
            premise,
            hypothesis,
            return_tensors="pt",
            padding=True,
            truncation=True,
        )
        input_ids = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)

        # Get logits
        with torch.no_grad():
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits[0]

        # Convert to probabilities
        probs = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()

        # Map to standard labels
        scores: dict[str, float] = {}
        for idx, prob in enumerate(probs):
            label = self._label_mapping.get(str(idx), str(idx))
            scores[label] = float(prob)

        # Ensure we have all three standard labels
        for label in ["entailment", "neutral", "contradiction"]:
            if label not in scores:
                scores[label] = 0.0

        # Cache result
        self.cache.set(
            self.model_name,
            "nli",
            scores,
            model_version=self.model_version,
            premise=premise,
            hypothesis=hypothesis,
        )

        return scores
